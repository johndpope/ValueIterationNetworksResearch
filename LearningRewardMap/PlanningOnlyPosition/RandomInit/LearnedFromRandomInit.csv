numstates; 10
 num value iterations; 30
 conv type; circular
conv width; 7
 channel_i; 1
 channel_h; 50
 channel_q; 15
hidden layer size; 30
 learning rate; 0.000100
 TAU; 0.001000
 batch size; 64
 reward; learned
 New angle formula
Episodes Spent Training; 10 Episode Eval Avg
200; -1033.2900799
300; -1039.3975138
400; -1268.29630265
500; -745.453226367
600; -1244.62756691
700; -1064.70709064
800; -1365.00770692
900; -1134.15490566
1000; -1347.843621
1100; -1057.40238437
1200; -1212.97970233
1300; -1135.14883441
1400; -1100.3847833
1500; -347.016104496
1600; -411.886346166
1700; -562.558591248
1800; -221.192512143
1900; -554.723249523
2000; -559.416327561
2100; -815.063192015
2200; -454.382448405
2300; -760.192718827
2400; -372.618755919
2500; -641.639268445
2600; -843.019356396
2700; -589.177984585
2800; -599.288365304
2900; -325.502436269
3000; -412.762640169
3100; -636.185171628
3200; -663.414272231
3300; -233.410891049
3400; -509.2855627
3500; -433.619491144
3600; -446.763652308
3700; -570.215021525
3800; -763.988072415
3900; -297.768091217
Time Training (4000episodes);20641.151251077652
Evaluation Episode; Reward
0; -911.167915964
1; -490.721963992
2; -246.352889266
3; -497.727508186
4; -494.898201632
5; -359.448932893
6; -621.418319769
7; -123.015555638
8; -248.890385999
9; -892.624052446
10; -124.948150964
11; -248.180027902
12; -246.701156877
13; -767.154139534
14; -1729.044366
15; -794.557835684
16; -1.75580147724
17; -888.069191343
18; -250.558236396
19; -490.586976594
20; -490.803503237
21; -613.214384309
22; -610.668260383
23; -368.213880798
24; -0.528262986151
25; -490.507664391
26; -125.24272848
27; -918.6868596
28; -123.183191165
29; -620.801770063
30; -632.756953718
31; -366.536569037
32; -1007.41782058
33; -245.405695807
34; -750.471491981
35; -120.288168201
36; -125.565438086
37; -124.010904502
38; -250.376509864
39; -770.823399747
40; -491.069732805
41; -121.878113465
42; -619.223833175
43; -0.470344758854
44; -244.151613779
45; -1.10060694944
46; -124.430686808
47; -1037.16103363
48; -252.646990908
49; -367.423667983
50; -251.459522239
51; -243.173258543
52; -622.516408374
53; -247.865815186
54; -125.310052359
55; -121.615445261
56; -615.815719686
57; -610.593625934
58; -252.620626043
59; -1.07665994604
60; -245.268564064
61; -489.889968615
62; -122.129017311
63; -491.720845939
64; -252.567931941
65; -904.631516547
66; -120.943411596
67; -969.885796547
68; -1090.1762774
69; -243.431894217
70; -632.342214882
71; -122.044630093
72; -1111.17207283
73; -117.988612006
74; -118.058922619
75; -0.808213229195
76; -247.439372275
77; -945.388930745
78; -251.588369521
79; -1730.88247517
80; -251.170600273
81; -367.729260604
82; -124.476733008
83; -370.318810562
84; -496.749004161
85; -367.514083811
86; -761.231473426
87; -873.115839956
88; -776.550063589
89; -123.775728113
90; -489.478788556
91; -495.599034421
92; -251.585959837
93; -368.059223395
94; -120.397580303
95; -609.019834098
96; -898.524257947
97; -1224.55181836
98; -366.193665121
99; -497.544787526
endExperiment
numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 7 
 channel_i; 1 
 channel_h; 50 
 channel_q; 15 
hidden layer size; 30 
 learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
 reward; 
 New angle formula 
Episodes Spent Training; 10 Episode Eval Avg 
200; -1116.1132721
300; -259.262305349
400; -523.361017472
500; -310.557816557
600; -593.855575705
700; -336.41771534
800; -374.975595173
900; -213.809318845
1000; -432.738594419
1100; -415.542832338
1200; -232.878399545
1300; -300.656733624
1400; -252.65181928
1500; -463.204615269
1600; -405.455026297
1700; -464.120883863
1800; -277.294351629
1900; -307.49223719
2000; -463.359057227
2100; -359.375541002
2200; -345.731695123
2300; -325.950257106
2400; -460.542257238
2500; -300.132573776
2600; -222.264886044
2700; -276.269259884
2800; -381.183525237
2900; -252.256631627
3000; -526.99418604
3100; -321.220625714
3200; -422.408393139
3300; -426.549971199
3400; -390.016080906
3500; -404.041791877
3600; -299.578877196
3700; -352.199060297
3800; -367.099657445
3900; -173.105014396
Time Training (4000episodes);20519.647902965546
Evaluation Episode; Reward 
0; -619.663778238
1; -764.287670028
2; -359.886611369
3; -623.872195094
4; -505.649041667
5; -127.011705375
6; -248.453929081
7; -118.925908619
8; -127.891317469
9; -126.956421347
10; -503.900821854
11; -495.401669156
12; -251.27772566
13; -127.758092261
14; -748.624319961
15; -0.0395225686647
16; -242.471566056
17; -123.56500425
18; -491.419683092
19; -125.441389215
20; -125.5688524
21; -123.065733777
22; -614.022091123
23; -0.174032998069
24; -249.14133601
25; -748.650216242
26; -500.377646003
27; -784.517179359
28; -364.540959855
29; -122.167014522
30; -248.002477449
31; -634.911353675
32; -674.980268365
33; -776.676913965
34; -125.532631299
35; -246.87410553
36; -125.286285971
37; -127.856740296
38; -127.513227945
39; -124.744906566
40; -719.324253137
41; -629.142023748
42; -127.734164396
43; -518.762174284
44; -358.112820227
45; -124.664162744
46; -124.523164684
47; -120.598120647
48; -736.529158309
49; -493.646061281
50; -758.271898559
51; -0.234831726497
52; -125.072774006
53; -126.783402633
54; -248.591946199
55; -122.774662151
56; -359.345272876
57; -123.272328265
58; -620.176439093
59; -623.412225227
60; -123.134492403
61; -0.473417270421
62; -506.170190351
63; -672.124182416
64; -125.400831676
65; -371.420628715
66; -499.490433969
67; -613.629260297
68; -125.625663874
69; -247.13015031
70; -126.864430174
71; -123.345210804
72; -126.868157554
73; -500.220393664
74; -125.586757111
75; -613.4540329
76; -123.700886071
77; -123.005265117
78; -363.164118733
79; -496.56314062
80; -612.526692467
81; -252.182273462
82; -0.0259632915275
83; -372.11325388
84; -251.28947483
85; -504.963752322
86; -248.817994741
87; -246.7293957
88; -368.435528217
89; -126.981403916
90; -124.354278684
91; -491.566411358
92; -244.106710204
93; -125.872203595
94; -244.875404314
95; -500.280450263
96; -498.783781279
97; -121.935684958
98; -250.570565887
99; -125.111214864
endExperiment

numstates; 1000 
 num value iterations; 30 
 conv type; circular 
conv width; 7 
 channel_i; 1 
 channel_h; 50 
 channel_q; 15 
hidden layer size; 30 
 learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
 reward; 
 New angle formula 
Episodes Spent Training; 10 Episode Eval Avg 
200; -1245.85713221
300; -1152.49161599
400; -623.78847621
500; -1240.57445424
600; -1217.21414205
700; -1000.1392018
800; -1228.79628081
900; -1116.41163536
1000; -1036.77906851
1100; -1271.46015654
1200; -1015.03420921
1300; -778.713922993
1400; -1087.84252131
1500; -1151.22427908
1600; -1016.44634095
1700; -1018.67170183
1800; -1078.00945612
1900; -924.513704877
2000; -1244.17487206
2100; -1245.3446343
2200; -888.407657989
2300; -1139.75842863
2400; -778.559954658
2500; -1030.92937771
2600; -979.60715692
2700; -1048.64802059
2800; -1584.79168754
2900; -884.825788522
3000; -971.957048805
3100; -997.979179381
3200; -918.971869777
3300; -876.544591026
3400; -1266.80466918
3500; -1087.91571757
3600; -1024.50222759
3700; -1086.88553951
3800; -1098.97419243
3900; -1304.4103679
Time Training (4000episodes);21156.46630334854
Evaluation Episode; Reward 
0; -1360.55025274
1; -1092.28065213
2; -1695.40157082
3; -1406.77631236
4; -810.514827625
5; -999.834269106
6; -0.387606276261
7; -1003.13714309
8; -1428.71565528
9; -1449.9788703
10; -1729.95614086
11; -128.414225629
12; -954.99116685
13; -255.48996016
14; -1209.56392036
15; -1393.96164017
16; -1114.8067703
17; -1233.4496588
18; -1734.31311659
19; -1731.26247208
20; -1735.4386752
21; -1197.54207443
22; -1655.16005569
23; -760.508189655
24; -1349.58907868
25; -1356.75641072
26; -1731.15244104
27; -878.840281454
28; -1617.38189617
29; -1372.33343616
30; -1171.55133483
31; -1312.30039637
32; -879.395187859
33; -759.787258862
34; -0.750567759575
35; -1469.62089682
36; -1378.9561964
37; -1250.9049597
38; -996.191818433
39; -1734.29710178
40; -1237.98513092
41; -0.812492466918
42; -1731.10209136
43; -1153.78968075
44; -1732.24510735
45; -1344.14167219
46; -0.471089632869
47; -1593.15321639
48; -1731.83394345
49; -1730.49881078
50; -127.038747141
51; -1433.35244329
52; -128.056400316
53; -1218.03653494
54; -877.477793228
55; -1055.81995898
56; -1732.55978945
57; -127.830413771
58; -1667.51691628
59; -1633.22909685
60; -1648.68286818
61; -1280.46902185
62; -128.024695728
63; -862.800011441
64; -880.983610232
65; -1080.05807769
66; -879.565487372
67; -1119.04064998
68; -1018.29651419
69; -128.54260852
70; -1311.90216271
71; -1350.09985543
72; -381.719503175
73; -1602.43060594
74; -0.673576278616
75; -912.595039544
76; -930.23130213
77; -1687.44071256
78; -1281.42264495
79; -1279.99826093
80; -896.249127814
81; -1288.34980567
82; -1011.40614902
83; -746.642768191
84; -1308.08773534
85; -1672.65868743
86; -999.567020158
87; -1413.6253568
88; -1104.61255888
89; -1170.3636289
90; -127.836656216
91; -1287.74560169
92; -0.41201091138
93; -1297.91061619
94; -1661.62760672
95; -0.382617765592
96; -1581.31938047
97; -1387.28966114
98; -1155.16821893
99; -975.383778046
endExperiment

numstates; 100 
 num value iterations; 100 
 conv type; circular 
conv width; 7 
 channel_i; 1 
 channel_h; 50 
 channel_q; 15 
hidden layer size; 30 
 learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
 reward; 
 New angle formula 
Episodes Spent Training; 10 Episode Eval Avg 
200; -1132.9990961
300; -431.379547159
400; -407.886307427
500; -281.012124461
600; -211.532639619
700; -223.55835453
800; -226.39100278
900; -249.760521342
1000; -211.024598095
1100; -332.157656151
1200; -333.478664021
1300; -377.951790119
1400; -341.499346089
1500; -364.565603297
1600; -321.14662127
1700; -308.596338014
1800; -359.972859033
1900; -356.326492693
2000; -333.022510848
2100; -210.951706735
2200; -249.431095318
2300; -364.347172497
2400; -366.838735171
2500; -394.112715014
2600; -367.254258036
2700; -340.619918547
2800; -320.600094903
2900; -370.149558417
3000; -283.942463233
3100; -234.104208627
3200; -263.967733315
3300; -354.054959234
3400; -451.356023708
3500; -364.196124693
3600; -376.622169289
3700; -344.360514291
3800; -435.142985714
3900; -476.491436753
Time Training (4000episodes);50152.37359046936
Evaluation Episode; Reward 
0; -0.978569539609
1; -704.675579537
2; -121.972874937
3; -504.525710143
4; -248.142590267
5; -623.691594457
6; -127.456305148
7; -502.326075931
8; -489.484715292
9; -0.712646058453
10; -118.223297038
11; -0.525657343071
12; -127.32314653
13; -618.262040806
14; -370.219901848
15; -123.490850127
16; -632.634096925
17; -125.617786031
18; -372.568266681
19; -625.289690147
20; -250.163116667
21; -708.714338966
22; -125.225572676
23; -245.55020074
24; -123.617212832
25; -769.216914622
26; -249.500150524
27; -124.334478257
28; -127.706427025
29; -745.675987077
30; -1.17108640727
31; -121.7392882
32; -126.106760648
33; -122.589562167
34; -248.911634612
35; -0.444040628921
36; -248.065860102
37; -125.561898322
38; -491.625512935
39; -123.237794783
40; -513.078073504
41; -126.330989698
42; -121.343020907
43; -368.281600342
44; -121.962889351
45; -122.248052128
46; -248.895815965
47; -506.252181862
48; -125.921751767
49; -125.812410637
50; -120.607064952
51; -122.928213748
52; -502.187912951
53; -123.686150906
54; -125.360268622
55; -126.877385482
56; -614.353963732
57; -612.549827274
58; -366.447917392
59; -119.118310498
60; -612.449751854
61; -125.003705642
62; -366.340774844
63; -124.60302747
64; -501.175445222
65; -613.527557554
66; -242.569486648
67; -126.619322951
68; -126.467908055
69; -124.328741557
70; -758.755026492
71; -363.460353252
72; -119.071149001
73; -120.574147297
74; -801.968877895
75; -122.502681158
76; -242.727543009
77; -498.055541206
78; -660.10799267
79; -631.350485429
80; -247.047204239
81; -643.968916703
82; -127.53669566
83; -123.080847594
84; -247.804025166
85; -774.335144457
86; -125.091780049
87; -506.959605477
88; -249.242777837
89; -125.944692394
90; -521.906414793
91; -123.971397109
92; -119.523517414
93; -124.219084493
94; -651.668858156
95; -124.237382681
96; -613.03470918
97; -127.056762978
98; -245.359709125
99; -491.606191819
endExperiment

numstates; 100 
 num value iterations; 10 
 conv type; circular 
conv width; 7 
 channel_i; 1 
 channel_h; 50 
 channel_q; 15 
hidden layer size; 30 
 learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
 reward; 
 New angle formula 
Episodes Spent Training; 10 Episode Eval Avg 
200; -960.91752834
300; -767.926458136
400; -895.473092462
500; -938.767899972
600; -827.155276056
700; -306.794383259
800; -816.335009934
900; -1041.05349872
1000; -472.196310048
1100; -818.267153031
1200; -708.221260123
1300; -1167.6756625
1400; -786.67952562
Time Training (1500episodes);3792.1043770313263
Evaluation Episode; Reward 
0; -1716.5870633
1; -870.738748623
2; -1311.76318111
3; -1.74045821147
4; -1457.91489469
5; -1665.53953545
6; -494.627037661
7; -1644.22828023
8; -1164.64630128
9; -1700.95969559
10; -360.250941711
11; -744.82733035
12; -119.744504613
13; -1681.83432404
14; -244.102915362
15; -246.664991749
16; -1636.76649767
17; -121.485696579
18; -251.00857885
19; -374.580164657
20; -493.182880717
21; -759.15880929
22; -1025.49953587
23; -744.921874652
24; -1731.1661947
25; -1702.58161907
26; -244.327514573
27; -1.19957765764
28; -119.709616002
29; -1418.80337569
30; -122.644949482
31; -1667.86267891
32; -745.316719593
33; -365.257697702
34; -619.162547456
35; -125.668473676
36; -495.419462576
37; -1653.00158783
38; -1718.74628038
39; -873.229859237
40; -494.535593968
41; -244.580097126
42; -491.182063979
43; -124.92635296
44; -126.53063245
45; -861.936042693
46; -125.774391149
47; -1.27446566452
48; -245.458956382
49; -369.219205397
50; -910.144114163
51; -120.050142805
52; -126.660774134
53; -370.981508395
54; -0.466216000287
55; -251.527520577
56; -743.163191522
57; -1691.35531772
58; -369.319011399
59; -120.203243592
60; -368.93193639
61; -244.930092588
62; -1340.09482408
63; -1021.52369602
64; -125.815880207
65; -126.476924019
66; -244.100962099
67; -358.386990463
68; -1.32310744593
69; -362.453041733
70; -366.605588622
71; -126.232927493
72; -1650.92822386
73; -0.326705536219
74; -874.204963629
75; -1730.92307281
76; -618.006226054
77; -126.902367384
78; -756.108965014
79; -741.756698735
80; -244.507387626
81; -244.243055787
82; -249.767748532
83; -492.858699449
84; -741.847053604
85; -1659.05102251
86; -607.536845623
87; -1017.36422483
88; -0.395327919985
89; -1428.1896273
90; -248.906887886
91; -125.931056935
92; -1235.53111654
93; -246.50409708
94; -0.53052701133
95; -124.113242314
96; -1730.43509584
97; -122.554011248
98; -491.627089188
99; -618.524892662
endExperiment

numstates; 100 
 num value iterations; 50 
 conv type; circular 
conv width; 7 
 channel_i; 1 
 channel_h; 50 
 channel_q; 15 
hidden layer size; 30 
 learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
 reward; 
 New angle formula 
Episodes Spent Training; 10 Episode Eval Avg 
200; -877.630375471
300; -1047.39175751
400; -1118.2317336
500; -946.901190542
600; -1353.11491978
700; -1444.89453392
800; -1245.62473751
900; -1219.04461355
1000; -1151.76295865
1100; -1163.83969992
1200; -834.584735512
1300; -773.686116721
1400; -1055.30050609
Time Training (1500episodes);10658.433889627457
Evaluation Episode; Reward 
0; -1350.06605119
1; -1451.17007371
2; -1312.04466603
3; -126.136514044
4; -381.010203722
5; -1726.25051981
6; -949.910318735
7; -1570.90887152
8; -1311.55835707
9; -1735.66700115
10; -994.152235983
11; -1201.48016524
12; -1729.52817957
13; -878.344001658
14; -127.454901507
15; -1120.78203791
16; -1099.70286977
17; -1011.49214916
18; -1689.79509403
19; -1129.43945557
20; -995.636417962
21; -1078.46865568
22; -1721.28126127
23; -1226.27570725
24; -1236.03327124
25; -127.407889392
26; -1016.39136271
27; -881.305733827
28; -128.553618056
29; -1722.29872531
30; -1321.79161224
31; -1000.72727407
32; -1070.47490907
33; -1071.94930932
34; -127.764670749
35; -1406.61583878
36; -0.230970391659
37; -1094.69403456
38; -1129.03974254
39; -1730.80755367
40; -127.78552199
41; -878.992792119
42; -1721.15472556
43; -758.986998763
44; -849.13629636
45; -785.757047351
46; -1338.30820841
47; -1109.19810423
48; -1506.56225133
49; -1121.0953294
50; -1335.72038658
51; -1472.08553555
52; -987.513492614
53; -1339.65884363
54; -1243.0066607
55; -1584.66824617
56; -126.737049967
57; -980.714820501
58; -1614.769348
59; -876.2485965
60; -1687.1689801
61; -1156.73723855
62; -1324.40179712
63; -1737.816283
64; -733.566945093
65; -881.4907375
66; -255.287588145
67; -758.005620279
68; -1207.70439011
69; -1569.21877935
70; -1429.51020584
71; -978.217042864
72; -877.843416211
73; -1328.15174643
74; -1460.84213982
75; -1130.22077877
76; -1080.6186976
77; -1467.64284967
78; -1287.1459364
79; -1329.87696509
80; -1144.92570802
81; -1613.41129424
82; -764.451334601
83; -1224.87893375
84; -772.31675441
85; -1038.49698485
86; -995.450317622
87; -759.788172718
88; -997.477446108
89; -1030.40916247
90; -1167.03347046
91; -1478.49358265
92; -126.90883036
93; -1443.25157465
94; -1709.80189319
95; -1275.03644216
96; -1004.8896193
97; -128.369723548
98; -1259.40193926
99; -1547.34409702
endExperiment

numstates; 100 
 num value iterations; 50 
 conv type; circular 
conv width; 7 
 channel_i; 1 
 channel_h; 50 
 channel_q; 15 
hidden layer size; 30 
 learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
 reward; 
 New angle formula 
Episodes Spent Training; 10 Episode Eval Avg 
200; -605.934374534
300; -263.291262179
400; -260.508270978
500; -471.746804476
600; -277.939685474
700; -445.625057781
800; -305.397006825
900; -368.65321161
1000; -411.699043888
1100; -254.635830763
1200; -362.002087078
1300; -274.454742717
1400; -246.538594305
1500; -251.988464654
1600; -238.909729273
1700; -310.33065811
1800; -378.091245998
1900; -284.367691707
2000; -375.495946826
2100; -475.162182317
2200; -349.154884357
2300; -359.504713643
2400; -249.950119699
2500; -370.315958331
2600; -425.521709142
2700; -333.624423555
2800; -390.117441535
2900; -226.418331881
3000; -341.008149846
3100; -334.927794886
3200; -197.548676325
3300; -397.818915969
3400; -270.551958221
3500; -280.665652349
3600; -483.694361879
3700; -317.716847959
3800; -166.514156951
3900; -286.890302171
Time Training (4000episodes);30041.283356189728
Evaluation Episode; Reward 
0; -123.418659803
1; -125.431089948
2; -246.502964191
3; -249.295230475
4; -123.354386453
5; -617.875904262
6; -126.187833439
7; -127.246663646
8; -367.791284561
9; -622.636345194
10; -371.111035815
11; -248.371429942
12; -245.140089865
13; -127.071371212
14; -714.434074479
15; -249.349187836
16; -126.091918364
17; -836.64128068
18; -368.319737249
19; -125.953748314
20; -374.619538867
21; -125.736108128
22; -490.349112884
23; -490.81003017
24; -119.418968183
25; -745.050775277
26; -126.766713041
27; -496.497981306
28; -0.309231574036
29; -125.88447579
30; -362.723308142
31; -124.565299616
32; -612.121321666
33; -376.372652038
34; -372.925118606
35; -242.975956151
36; -504.333250787
37; -124.057176342
38; -496.556604991
39; -639.239259275
40; -244.359744499
41; -128.060809999
42; -762.207399432
43; -126.461561533
44; -120.817518655
45; -120.66520748
46; -367.669657538
47; -121.756220511
48; -696.796364022
49; -0.522864356397
50; -622.61157298
51; -247.336664343
52; -607.670936326
53; -618.485391244
54; -243.43944589
55; -122.685377649
56; -625.114779939
57; -496.195941727
58; -244.562552982
59; -126.912702653
60; -125.173170829
61; -119.063759515
62; -246.394284715
63; -248.574971126
64; -0.728332549461
65; -126.311627579
66; -360.119521736
67; -374.941954439
68; -251.601046911
69; -246.96238806
70; -636.533191426
71; -125.580873899
72; -126.242728245
73; -369.600011411
74; -248.579137733
75; -122.155318588
76; -495.58969578
77; -125.691498506
78; -125.80832813
79; -602.941699484
80; -501.266556556
81; -126.030898894
82; -376.119908946
83; -124.743076182
84; -0.419099606796
85; -831.133193205
86; -363.458910805
87; -250.878669207
88; -127.017228619
89; -0.897903192092
90; -242.690504923
91; -509.67334388
92; -368.234477891
93; -504.344346011
94; -492.601814793
95; -0.382958787091
96; -1.01987544899
97; -810.346071835
98; -489.94295891
99; -502.240983395
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 3 
 channel_i; 1 
 channel_h; 50 
 channel_q; 15 
hidden layer size; 30 
 learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
 reward; 
 New angle formula 
Episodes Spent Training; 10 Episode Eval Avg 
200; -1256.82729639
300; -774.686195496
400; -425.881829455
500; -519.835837299
600; -662.055798432
700; -636.452426517
800; -539.578387895
900; -505.967389342
1000; -796.01257978
1100; -499.277569008
1200; -481.595487741
1300; -471.944278964
1400; -703.139255304
1500; -480.712341215
1600; -358.524268297
1700; -470.433437845
1800; -428.733960538
1900; -486.69460314
2000; -503.948677501
2100; -371.783835182
2200; -611.824491888
2300; -511.493747948
2400; -482.907014596
2500; -835.619103792
2600; -515.825988946
2700; -393.857636083
2800; -496.535328358
2900; -543.975328081
3000; -760.828799026
3100; -709.027832047
3200; -536.885269491
3300; -454.160312761
3400; -607.667320212
3500; -422.694516045
3600; -370.36662689
3700; -910.3743696
3800; -709.491354106
3900; -533.108036712
Time Training (4000episodes);19705.67382836342
Evaluation Episode; Reward 
0; -370.155654299
1; -1157.17930618
2; -126.381097777
3; -365.925466247
4; -124.627476119
5; -123.686274465
6; -1734.64633353
7; -251.461806632
8; -125.648527601
9; -370.700533788
10; -491.176209073
11; -249.710668937
12; -125.647505086
13; -881.928905897
14; -1.30508344335
15; -366.181440706
16; -614.731716726
17; -1729.50701242
18; -869.604388685
19; -1733.3212989
20; -126.94583546
21; -251.253092752
22; -125.611216277
23; -247.188544475
24; -1137.46024178
25; -1111.66388004
26; -124.253476594
27; -1027.60982797
28; -252.471316149
29; -1729.35847366
30; -615.665960121
31; -369.104279551
32; -247.989934688
33; -497.185955253
34; -251.690736187
35; -125.787856137
36; -247.228006382
37; -498.739861946
38; -614.1116748
39; -250.667926074
40; -1162.42047983
41; -125.452811516
42; -756.835941343
43; -620.792413363
44; -251.470826725
45; -1172.63824553
46; -251.764346603
47; -124.916248512
48; -496.111575409
49; -247.261396104
50; -777.390313306
51; -249.757574532
52; -126.150692317
53; -869.002107646
54; -1092.12566927
55; -126.207594298
56; -491.66114472
57; -126.440313146
58; -1049.47762272
59; -492.123159445
60; -366.311313356
61; -124.122608553
62; -126.813738779
63; -124.879661908
64; -376.563801424
65; -495.797966835
66; -126.399245224
67; -607.625834396
68; -949.333773062
69; -251.789430394
70; -126.833227935
71; -123.222085403
72; -1.17876159338
73; -894.795523833
74; -127.904922969
75; -364.489549726
76; -248.320282389
77; -1721.93283072
78; -124.117571282
79; -251.792777359
80; -369.39197613
81; -250.95563573
82; -877.213159209
83; -122.337495111
84; -756.478494008
85; -1728.10273621
86; -123.199266124
87; -126.470546937
88; -251.687249682
89; -1052.9971416
90; -374.495432457
91; -375.912756346
92; -245.27533591
93; -250.490962835
94; -888.150138007
95; -126.733568374
96; -1052.87091494
97; -1033.75369756
98; -249.637875639
99; -748.16997964
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_h; 50 
 channel_q; 15 
hidden layer size; 30 
 learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
 reward; 
 New angle formula 
Episodes Spent Training; 10 Episode Eval Avg 
200; -853.339739065
300; -867.869507842
400; -376.096751603
500; -598.186221578
600; -586.231421125
700; -477.849465632
800; -879.376021302
900; -395.010798612
1000; -572.13806344
1100; -785.850964687
1200; -287.182252585
1300; -496.840194893
1400; -950.221132076
1500; -553.523641731
1600; -611.206255476
1700; -407.744236363
1800; -508.460049668
1900; -627.788724402
2000; -667.490154857
2100; -532.681126643
2200; -1016.62266836
2300; -732.869879999
2400; -431.060846532
2500; -891.021975744
2600; -597.697452466
2700; -632.918052477
2800; -806.394451921
2900; -585.331051737
3000; -570.316551175
3100; -426.146968056
3200; -480.926570861
3300; -578.928212747
3400; -765.455663446
3500; -712.1679465
3600; -776.819814295
3700; -835.650475043
3800; -572.56595314
3900; -559.969691934
Time Training (4000episodes);20568.75785779953
Evaluation Episode; Reward 
0; -494.892757753
1; -1.07758986713
2; -745.080106543
3; -494.580138357
4; -126.775500236
5; -1731.14137673
6; -1.25370929576
7; -492.101536855
8; -124.393187553
9; -127.280038358
10; -492.375621449
11; -252.103142353
12; -251.365063413
13; -880.944784085
14; -126.880029957
15; -248.250403587
16; -1729.38514686
17; -883.738430388
18; -252.015326022
19; -1.35029201674
20; -1473.36858383
21; -1671.83732505
22; -249.020466206
23; -122.951531145
24; -372.454394274
25; -744.881319211
26; -868.059730409
27; -124.996785883
28; -1.1584743488
29; -1735.87974019
30; -366.545207093
31; -764.833297253
32; -251.671429895
33; -1.3213634652
34; -125.347940617
35; -366.3706608
36; -608.68177931
37; -858.229257712
38; -249.62142397
39; -252.451700109
40; -757.848830803
41; -126.63406935
42; -1163.17863973
43; -250.619232685
44; -126.313726662
45; -1.41476516627
46; -364.594067682
47; -249.383259882
48; -1207.16089816
49; -121.18740966
50; -122.99417684
51; -496.504667734
52; -123.855411749
53; -249.70280393
54; -123.219696741
55; -248.635109625
56; -124.637905439
57; -606.227130392
58; -367.520601396
59; -493.171900122
60; -250.186138005
61; -125.275526649
62; -126.756967898
63; -612.567435102
64; -1.09220871728
65; -125.800956234
66; -369.511310266
67; -251.203637706
68; -1389.98448937
69; -375.584531079
70; -1178.61573936
71; -1307.28301129
72; -248.96913808
73; -495.652022341
74; -1.37145110616
75; -498.593908236
76; -1167.1015678
77; -125.889524019
78; -491.008513851
79; -251.070597573
80; -126.520258539
81; -247.814088295
82; -1008.94786677
83; -124.746856759
84; -364.97043312
85; -1027.70256086
86; -1184.29656301
87; -121.572456305
88; -249.354201746
89; -1730.23692305
90; -362.956287507
91; -496.858919748
92; -249.517073197
93; -1.34087606984
94; -248.525246119
95; -1.23522242752
96; -122.832376707
97; -125.344528845
98; -1041.90476365
99; -1334.38505181
endExperiment

