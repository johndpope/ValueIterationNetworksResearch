numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 256 
Episodes Spent Training; 50 Episode Eval Avg 
199; -344.773954761
299; -258.69403709
399; -310.02861923
499; -232.110691848
599; -263.353735254
699; -257.399455003
799; -249.99625943
899; -239.664145165
999; -238.747256934
1099; -292.689885023
1199; -283.687831825
1299; -238.705433541
1399; -301.546323907
1499; -246.157973735
1599; -259.051249769
1699; -245.602030085
1799; -237.240749631
1899; -258.401187681
1999; -284.350777961
2099; -208.918113342
2199; -227.342462135
2299; -276.949321831
2399; -231.337235351
2499; -238.502770182
2599; -252.63930919
2699; -246.784108427
2799; -286.358393117
2899; -230.574201309
2999; -250.669809932
3099; -272.18313023
3199; -239.770521169
3299; -227.219259078
3399; -287.389706692
3499; -257.855705958
3599; -261.393688101
3699; -241.272583765
3799; -277.633040254
3899; -228.469659163
3999; -259.460649696
Time Training (4000episodes);29654.58218717575
Evaluation Episode; Reward 
0; -366.614677365
1; -124.239292265
2; -125.469229736
3; -117.869292412
4; -640.977989629
5; -246.440187484
6; -124.463925776
7; -245.287727353
8; -623.484781891
9; -119.270155005
10; -362.835851471
11; -121.449917321
12; -241.706017184
13; -126.514852241
14; -125.810533354
15; -505.292034093
16; -124.376085846
17; -234.36840014
18; -122.00112144
19; -121.165057493
20; -355.211106489
21; -355.441424619
22; -360.922012412
23; -243.460933753
24; -234.816520586
25; -245.101706851
26; -501.923488583
27; -497.992712643
28; -606.643565542
29; -119.953408851
30; -123.466167468
31; -120.817545682
32; -243.206212153
33; -124.085941488
34; -366.600050495
35; -125.517441675
36; -351.986271745
37; -120.418217756
38; -494.243004199
39; -243.403020622
40; -554.956346246
41; -125.452360346
42; -0.389152682005
43; -369.136153419
44; -124.048486182
45; -238.475154531
46; -119.084539806
47; -123.244890293
48; -366.607175609
49; -119.898224122
50; -244.035414234
51; -233.005588113
52; -124.754122325
53; -0.273457468918
54; -121.703440002
55; -660.989243778
56; -123.794266196
57; -239.697861411
58; -123.953460141
59; -120.323843442
60; -121.849863361
61; -352.831880301
62; -364.045033644
63; -122.184200716
64; -237.070951122
65; -123.744265358
66; -124.335786935
67; -505.214430609
68; -238.359509845
69; -243.50699359
70; -353.190766519
71; -121.529057015
72; -235.513180568
73; -237.731939229
74; -124.199395128
75; -641.795523239
76; -120.367124762
77; -0.368428083576
78; -240.614049788
79; -123.998304036
80; -238.76857563
81; -0.328906782022
82; -362.236995765
83; -352.359770032
84; -122.438311144
85; -243.613991546
86; -241.075542995
87; -235.426483958
88; -121.026209548
89; -245.59388599
90; -238.480488901
91; -366.541655636
92; -360.847293368
93; -237.088485405
94; -363.523107294
95; -122.668951416
96; -122.117434492
97; -474.668836381
98; -240.957408886
99; -594.342950899
endExperiment

numstates; 500 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 256 
Episodes Spent Training; 50 Episode Eval Avg 
199; -341.775159189
299; -296.771170553
399; -237.571525126
499; -277.504817871
Time Training (500episodes);8602.420198202133
Evaluation Episode; Reward 
0; -1.07526640969
1; -127.822346917
2; -505.144454795
3; -125.431434885
4; -122.728737711
5; -125.236046437
6; -365.059766906
7; -126.498524603
8; -558.928001784
9; -237.824921213
10; -1.12924713405
11; -373.391784438
12; -123.834750604
13; -372.859933139
14; -126.96319241
15; -1.11978455974
16; -120.05439167
17; -123.105685833
18; -124.838632752
19; -120.284483307
20; -491.724397322
21; -246.575035956
22; -123.176128592
23; -356.806846666
24; -123.129224297
25; -123.787716351
26; -125.776142806
27; -374.184219923
28; -121.290752157
29; -513.24602071
30; -621.330833172
31; -1.13452005171
32; -125.903155107
33; -238.077670436
34; -480.952606037
35; -548.059874429
36; -120.317147443
37; -121.379621348
38; -1.1979490156
39; -481.683296122
40; -372.01651679
41; -370.464905316
42; -611.315591995
43; -127.168605849
44; -236.432406006
45; -590.035545593
46; -121.564800067
47; -122.960068424
48; -123.663381319
49; -601.848783626
50; -245.809926214
51; -1.25535401116
52; -358.532369475
53; -126.95886501
54; -120.542045572
55; -240.277306326
56; -126.191480695
57; -1.1287300268
58; -1.08635543873
59; -237.827401564
60; -127.310336693
61; -367.868928507
62; -247.207984838
63; -124.970646903
64; -511.563652135
65; -123.721095964
66; -351.126418756
67; -124.228085393
68; -120.039920762
69; -524.285844166
70; -124.958545141
71; -122.602487179
72; -616.121519595
73; -243.814232772
74; -489.371210348
75; -125.16978058
76; -528.144418348
77; -125.259674802
78; -532.152358238
79; -599.439046317
80; -126.585792196
81; -123.66908312
82; -1.4322015603
83; -125.992711556
84; -124.611324294
85; -125.971755274
86; -598.324276751
87; -240.744125563
88; -606.213680326
89; -121.015839344
90; -528.734628611
91; -356.788492388
92; -242.322331925
93; -245.696209268
94; -239.668964326
95; -126.311922374
96; -373.930495465
97; -610.631650604
98; -509.270919614
99; -125.180663975
endExperiment

numstates; 10 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
numstates; 10 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -607.445814075
299; -346.919569009
399; -370.338579336
499; -314.948672164
599; -338.178151268
699; -220.030857931
799; -245.157942922
899; -247.776141682
999; -245.739850789
1099; -270.431014707
1199; -249.594372148
1299; -250.495350506
1399; -254.604303059
1499; -264.980118506
1599; -248.235323156
1699; -232.879457042
1799; -211.840049027
1899; -229.955807179
1999; -282.624526582
2099; -257.197969905
2199; -290.544005405
2299; -204.86044746
2399; -270.400283884
2499; -247.839977838
2599; -286.908085777
2699; -265.384397816
2799; -253.214204669
2899; -195.130593476
2999; -288.631910084
3099; -238.537450681
3199; -252.229447788
3299; -268.40116589
3399; -264.865187483
3499; -241.441553638
3599; -230.957253219
3699; -298.241727313
3799; -235.612312127
3899; -282.133962123
3999; -279.40341109
Time Training (4000episodes);22575.38431096077
Evaluation Episode; Reward 
0; -125.199198341
1; -367.836988165
2; -125.729777464
3; -125.576626594
4; -124.327337283
5; -1.12327191834
6; -126.020315063
7; -121.121389408
8; -353.494092798
9; -497.941867393
10; -240.178703384
11; -122.114946539
12; -235.380897367
13; -241.282339354
14; -123.855281065
15; -1.12698522841
16; -592.241952561
17; -372.341577082
18; -126.562464916
19; -493.448787128
20; -642.665013828
21; -121.106462061
22; -500.806109805
23; -124.698521768
24; -242.127917684
25; -121.780472046
26; -125.784138557
27; -123.607820144
28; -237.727854489
29; -358.285012644
30; -246.176815796
31; -506.663764265
32; -125.199623363
33; -484.993259613
34; -365.503535465
35; -350.959407606
36; -364.068518723
37; -125.10396314
38; -237.54774943
39; -125.466839022
40; -246.686400439
41; -1.02947885773
42; -121.367734764
43; -123.047968311
44; -120.35389794
45; -120.27452183
46; -240.320856635
47; -509.957419377
48; -236.731381622
49; -125.195912474
50; -125.752027501
51; -125.225415926
52; -124.308848029
53; -477.303939169
54; -543.290307729
55; -238.666338395
56; -125.892369032
57; -471.635884428
58; -481.609472356
59; -358.131069397
60; -123.94945639
61; -126.462653512
62; -0.90982050106
63; -352.742968418
64; -560.747271354
65; -1.35902695266
66; -125.452771961
67; -475.342284208
68; -234.5084423
69; -553.550010979
70; -126.084419029
71; -238.244446825
72; -508.106984188
73; -121.971095245
74; -121.980797866
75; -640.860441438
76; -121.797973315
77; -127.23923009
78; -126.413791505
79; -501.269735786
80; -1.11918204772
81; -239.810544844
82; -353.602287874
83; -119.783285038
84; -623.706279412
85; -367.755903604
86; -125.161551543
87; -125.896643359
88; -239.825935865
89; -607.650872252
90; -356.263069914
91; -575.681233133
92; -506.295565194
93; -125.111789688
94; -491.137427314
95; -234.117264107
96; -126.514780607
97; -125.722701697
98; -367.139798304
99; -124.948030299
endExperiment

numstates; 500 
 num value iterations; 10 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -274.456687372
299; -272.07949621
399; -282.801275372
499; -223.47666042
599; -227.341858164
699; -250.274465734
799; -259.330988051
899; -237.357464453
999; -237.196048939
Time Training (1000episodes);7299.220938444138
Evaluation Episode; Reward 
0; -123.609087386
1; -125.452588888
2; -244.976920001
3; -496.090270732
4; -121.322332222
5; -245.597033426
6; -240.118732242
7; -123.153356439
8; -353.591282899
9; -125.278004955
10; -490.263231206
11; -376.001154432
12; -510.867899793
13; -122.287941096
14; -126.033812789
15; -125.10817971
16; -241.839117335
17; -118.740019309
18; -0.786115644993
19; -121.159283938
20; -488.701110569
21; -119.937373788
22; -524.457748021
23; -576.739357885
24; -572.976158023
25; -0.752018629452
26; -122.67946291
27; -472.633438922
28; -543.349100378
29; -384.890021991
30; -119.273447214
31; -124.561050791
32; -122.485072605
33; -125.557021936
34; -540.668792567
35; -124.734884998
36; -125.629515039
37; -581.699392118
38; -124.47665962
39; -123.059046816
40; -123.973154616
41; -121.989532684
42; -238.606291733
43; -0.779791105998
44; -635.722859606
45; -124.027386926
46; -358.979284442
47; -574.057664194
48; -235.951276409
49; -125.721962357
50; -0.884664842107
51; -476.762328419
52; -245.529115741
53; -622.920953525
54; -119.75854033
55; -125.873323171
56; -243.099473598
57; -357.256797616
58; -530.136110707
59; -121.242071378
60; -122.195457848
61; -238.248680709
62; -619.203105888
63; -629.605909208
64; -123.366378783
65; -372.071761732
66; -246.498036606
67; -1.34690640515
68; -125.198986869
69; -357.493104363
70; -354.311912793
71; -362.527317702
72; -122.996434195
73; -533.91808575
74; -120.916051019
75; -358.691930145
76; -242.908822609
77; -124.859730455
78; -0.850425323155
79; -124.643479052
80; -357.417600393
81; -367.514939489
82; -126.854320727
83; -489.25538331
84; -240.61162966
85; -351.589060453
86; -122.287125805
87; -125.879239103
88; -124.929837967
89; -360.656046644
90; -238.781781085
91; -569.025774221
92; -490.79797452
93; -353.85002549
94; -245.96415933
95; -247.982138591
96; -0.988616109668
97; -498.496060605
98; -122.199850884
99; -117.865985891
endExperiment

numstates; 500 
 num value iterations; 50 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -407.2819899
299; -539.276023931
399; -269.929722371
499; -237.34085541
599; -284.196780771
699; -371.641788359
799; -255.46234065
899; -228.458741139
999; -301.126175799
Time Training (1000episodes);29117.130209684372
Evaluation Episode; Reward 
0; -126.245778341
1; -363.28117737
2; -122.244430222
3; -121.962547192
4; -125.290892351
5; -119.022027183
6; -244.220313966
7; -581.666022577
8; -121.957632031
9; -119.0409403
10; -235.969577902
11; -0.905609244636
12; -351.349344335
13; -118.100943881
14; -504.218209929
15; -475.093434631
16; -122.068396445
17; -117.978192135
18; -473.479719661
19; -697.922527215
20; -237.38809814
21; -120.825990393
22; -242.466352803
23; -369.020351158
24; -119.218810562
25; -590.766784137
26; -233.827787769
27; -242.610161108
28; -369.911203451
29; -366.775915673
30; -573.973454124
31; -118.920990969
32; -368.204914718
33; -0.14442756765
34; -124.081425943
35; -592.669469742
36; -0.121113888306
37; -125.104914233
38; -119.851269437
39; -0.0836452806643
40; -599.941046
41; -243.642150542
42; -470.398261752
43; -122.493901556
44; -242.89337533
45; -123.403206585
46; -125.747776098
47; -122.033860295
48; -359.410808969
49; -508.662413488
50; -122.397043245
51; -124.654772279
52; -243.391240889
53; -364.693240018
54; -502.98328723
55; -121.290428456
56; -121.278370156
57; -121.670726864
58; -361.732624485
59; -125.50104842
60; -362.533731319
61; -125.046699655
62; -125.345701673
63; -365.335387739
64; -368.765584659
65; -119.68859675
66; -121.348203727
67; -598.410447989
68; -122.115975584
69; -125.317451291
70; -631.855653054
71; -554.328379121
72; -121.308090664
73; -361.570081412
74; -492.426488466
75; -475.034119076
76; -241.952807659
77; -368.157645561
78; -125.015648164
79; -125.940642486
80; -118.540400631
81; -124.733926959
82; -121.8034726
83; -351.829122498
84; -705.299019854
85; -241.035181466
86; -244.961629983
87; -235.202067577
88; -0.0605645651814
89; -126.007614433
90; -355.635367302
91; -123.568054625
92; -122.582592569
93; -120.870828298
94; -242.973036435
95; -119.660947947
96; -121.350746965
97; -123.705480774
98; -0.239817691252
99; -359.695058369
endExperiment

