numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 256 
Episodes Spent Training; 50 Episode Eval Avg 
199; -263.211366303
Time Training (1000episodes);1123.4115359783173
Evaluation Episode; Reward 
0; -254.459133079
1; -756.975436847
2; -635.87851536
3; -123.580521365
4; -123.912611506
5; -122.067414013
6; -126.39258656
7; -477.847684084
8; -0.23780762443
9; -367.036453877
10; -123.881701456
11; -241.258689764
12; -244.536718184
13; -244.668822989
14; -124.575491808
15; -683.489910436
16; -577.484216503
17; -593.766934011
18; -248.383299354
19; -120.640522583
20; -356.161990812
21; -246.472936386
22; -245.875141168
23; -123.645913805
24; -352.758059364
25; -500.69033388
26; -358.22247535
27; -236.517611418
28; -240.075348096
29; -504.528843536
30; -124.376110689
31; -123.534225189
32; -126.257763708
33; -474.843157107
34; -122.976401418
35; -125.896676322
36; -122.031044748
37; -240.975343799
38; -125.306839226
39; -0.965417199948
40; -605.712599356
41; -121.125269958
42; -121.979079568
43; -126.558851816
44; -126.512119355
45; -125.80816604
46; -125.058157562
47; -358.90995993
48; -369.954597976
49; -242.368905255
50; -237.207780228
51; -553.331781157
52; -126.091180073
53; -125.604834842
54; -356.581155802
55; -613.17892195
56; -482.287186637
57; -118.07500271
58; -125.952366587
59; -126.335437766
60; -124.722020903
61; -125.210361753
62; -121.049376116
63; -124.063969457
64; -254.332579615
65; -603.076234721
66; -360.504029072
67; -360.898378611
68; -572.005238932
69; -125.833498768
70; -247.829176499
71; -124.495611449
72; -254.203390998
73; -875.203211672
74; -840.122615753
75; -684.004749895
76; -368.789883743
77; -0.150082253797
78; -238.906153855
79; -126.488184257
80; -242.132010512
81; -367.506859191
82; -359.875172384
83; -119.205532721
84; -508.322293739
85; -369.516903391
86; -125.985348363
87; -460.568614746
88; -121.076752228
89; -542.673312288
90; -359.04169023
91; -0.24670528617
92; -122.44465457
93; -125.45628231
94; -598.423430825
95; -125.677061558
96; -369.831659713
97; -583.35110689
98; -126.989703138
99; -244.147002089
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 256 
Episodes Spent Training; 50 Episode Eval Avg 
199; -372.751207721
Time Training (1000episodes);1105.5861399173737
Evaluation Episode; Reward 
0; -124.76998299
1; -1207.11883331
2; -475.540716006
3; -650.455068779
4; -243.487443108
5; -914.265672068
6; -125.442771637
7; -707.156371975
8; -125.975291339
9; -124.801813163
10; -120.318720021
11; -377.380146951
12; -364.1455118
13; -501.225010236
14; -1043.71930628
15; -360.544392639
16; -594.888422844
17; -122.211080511
18; -604.202182366
19; -673.22177991
20; -0.461686322095
21; -362.543372006
22; -247.308704787
23; -498.999101129
24; -126.785408744
25; -652.023090038
26; -126.456478562
27; -124.547172138
28; -371.460020358
29; -365.361397423
30; -243.63714807
31; -250.259059076
32; -478.403998389
33; -123.733340594
34; -242.621054569
35; -615.812619715
36; -488.854777776
37; -360.797194236
38; -362.812833358
39; -368.305006997
40; -603.629519329
41; -123.221492397
42; -365.812726212
43; -249.600365375
44; -637.046618255
45; -0.289922822301
46; -124.632533334
47; -480.120144043
48; -765.809792234
49; -239.963237594
50; -494.277884751
51; -502.1031595
52; -250.662802218
53; -373.71910716
54; -672.967421755
55; -607.914660925
56; -245.952481034
57; -127.007212987
58; -367.872356806
59; -488.423358178
60; -747.237759784
61; -126.590861494
62; -363.203523096
63; -1094.4044176
64; -123.666309979
65; -241.180200598
66; -369.809642577
67; -243.997525204
68; -124.748908968
69; -245.4257015
70; -123.339267007
71; -359.183473613
72; -1182.67166191
73; -244.534492474
74; -489.464544018
75; -124.931484139
76; -476.226158762
77; -358.832121213
78; -605.651117774
79; -369.212183867
80; -122.361719701
81; -243.142204347
82; -244.892234538
83; -1179.02670796
84; -1031.21310743
85; -594.207456489
86; -125.065031547
87; -1078.28701424
88; -121.045572503
89; -245.609919477
90; -241.099043371
91; -244.774070969
92; -772.615761126
93; -248.880675981
94; -245.883254319
95; -124.955307034
96; -482.793482267
97; -488.381049931
98; -362.5077231
99; -122.078277551
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 256 
Episodes Spent Training; 50 Episode Eval Avg 
199; -599.821827147
299; -213.849729297
Time Training (1000episodes);1835.5360465049744
Evaluation Episode; Reward 
0; -489.123875793
1; -477.620299588
2; -121.319634901
3; -0.937486518935
4; -124.191598725
5; -239.758183469
6; -126.089637646
7; -119.844633483
8; -244.989555896
9; -368.296358595
10; -242.976702862
11; -495.592331572
12; -356.38208549
13; -121.339490619
14; -483.375712518
15; -509.116120602
16; -126.100660177
17; -369.517728327
18; -482.450432861
19; -126.305184421
20; -243.992140317
21; -127.318923686
22; -127.467470462
23; -606.924589243
24; -365.803931738
25; -367.662657388
26; -238.068952648
27; -118.498689122
28; -123.32591782
29; -124.567886117
30; -479.714199711
31; -119.730114881
32; -491.564674835
33; -244.462703811
34; -370.164924953
35; -360.077723869
36; -123.373639208
37; -126.540111605
38; -377.772064139
39; -125.748379065
40; -1.39922263292
41; -121.324853688
42; -250.076564475
43; -123.992017789
44; -126.966428527
45; -127.156629181
46; -117.834237012
47; -123.173939628
48; -125.320285326
49; -126.820376444
50; -510.134911573
51; -120.556837008
52; -245.893054975
53; -121.973700514
54; -1.67509307364
55; -241.77410542
56; -125.186370201
57; -350.301599823
58; -240.024551356
59; -127.67758946
60; -497.741369487
61; -358.757987012
62; -361.830960056
63; -362.698604388
64; -125.036155544
65; -568.623119165
66; -125.75497702
67; -1.01381611831
68; -126.96362772
69; -480.973504055
70; -533.12578343
71; -239.619011569
72; -242.937113922
73; -246.785750561
74; -123.460786174
75; -126.521744804
76; -357.982458061
77; -370.462689878
78; -124.196450364
79; -601.816184113
80; -238.85423044
81; -356.304695983
82; -241.138570693
83; -122.657983353
84; -380.782025959
85; -0.980507106999
86; -242.299658466
87; -124.497502888
88; -246.2644273
89; -238.984013747
90; -123.550460921
91; -361.773726462
92; -475.038200604
93; -124.81949996
94; -254.643944948
95; -255.978512958
96; -124.824898218
97; -1.04447810838
98; -235.095357099
99; -599.994827799
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
learning rate; 0.000100 
 TAU; 0.001000 
 batch size; 256 
Episodes Spent Training; 50 Episode Eval Avg 
199; -438.061791487
Time Training (1000episodes);1111.3593821525574
Evaluation Episode; Reward 
0; -771.829883521
1; -871.394533869
2; -251.281110815
3; -490.13855203
4; -124.937249995
5; -127.99015122
6; -126.127354735
7; -1.14313065779
8; -875.172152439
9; -245.795014019
10; -374.797044347
11; -246.953505956
12; -124.315539013
13; -250.431071925
14; -0.511989747558
15; -252.409555451
16; -485.232006389
17; -376.138103188
18; -0.90101826827
19; -244.490270032
20; -0.283386734503
21; -251.551163205
22; -619.609941237
23; -487.864958235
24; -375.611787408
25; -484.690012808
26; -618.482062222
27; -1729.41099797
28; -124.832923781
29; -126.026464202
30; -494.682742107
31; -771.884738694
32; -246.629788449
33; -620.293891585
34; -603.084903613
35; -250.930255099
36; -250.137736215
37; -898.356373289
38; -903.239222165
39; -775.022666933
40; -375.679288983
41; -487.607026306
42; -370.224488659
43; -489.120907532
44; -994.554420365
45; -647.547288873
46; -371.025922242
47; -124.651196268
48; -125.156337615
49; -486.373385996
50; -504.954523926
51; -124.913222247
52; -377.479291018
53; -0.357714870736
54; -126.969189628
55; -371.387230137
56; -251.180108477
57; -677.468699313
58; -125.226411541
59; -126.089112681
60; -509.316062348
61; -726.057202255
62; -250.296417877
63; -125.491142093
64; -0.313470677826
65; -124.854155796
66; -916.640316611
67; -123.51455852
68; -373.774473687
69; -124.154404954
70; -122.445345777
71; -126.326210986
72; -881.520115387
73; -700.681796142
74; -123.281272228
75; -680.110230808
76; -625.364159833
77; -632.841231316
78; -127.518714278
79; -369.744351088
80; -246.908231394
81; -487.202931374
82; -498.120381024
83; -736.763321563
84; -128.08331324
85; -689.679295811
86; -782.677451178
87; -378.229569311
88; -126.120168498
89; -485.128551333
90; -747.085256669
91; -909.259496126
92; -652.324187447
93; -683.690893076
94; -128.809399966
95; -503.576993003
96; -372.424882671
97; -365.594445911
98; -246.374012394
99; -731.023743635
endExperiment

