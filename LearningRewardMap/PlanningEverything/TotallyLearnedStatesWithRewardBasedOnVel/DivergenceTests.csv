numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -1311.16362372
299; -845.392344956
399; -290.739172568
499; -312.221211105
numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -429.777245758
299; -345.300307926
399; -470.381682088
499; -424.150613717
Time Training (500episodes);3194.4596524238586
Evaluation Episode; Reward 
0; -765.776448375
1; -364.610833454
2; -244.049386707
3; -605.193749412
4; -357.468825102
5; -121.094984655
6; -127.077592169
7; -1.22813893668
8; -353.115860503
9; -359.747009042
10; -244.111686309
11; -761.345517938
12; -511.822167015
13; -356.019137979
14; -684.410009863
15; -238.420854341
16; -367.592501595
17; -606.216085298
18; -2.26582644005
19; -122.425939429
20; -358.886340108
21; -241.415120257
22; -767.941074844
23; -123.3719198
24; -764.567955324
25; -126.581391631
26; -768.11833121
27; -517.036749245
28; -127.598796928
29; -771.53358832
30; -126.85896738
31; -363.28067261
32; -245.166312523
33; -239.729026365
34; -767.831378261
35; -127.212599541
36; -126.35311509
37; -3.35907256802
38; -126.587239869
39; -127.191780475
40; -242.441361439
41; -126.436235332
42; -235.436326923
43; -246.66391423
44; -483.247874283
45; -121.321313592
46; -505.207936608
47; -483.80950953
48; -119.62141751
49; -354.593498676
50; -625.297084155
51; -126.237904182
52; -503.531830972
53; -1.71026866526
54; -518.282020875
55; -768.596185789
56; -1729.06009159
57; -125.24544733
58; -355.772832322
59; -124.818634899
60; -772.560383677
61; -485.534488081
62; -355.18629169
63; -124.651546757
64; -368.281697011
65; -3.49901563636
66; -238.689100119
67; -771.29093102
68; -1.80465420982
69; -356.746444693
70; -1730.96976715
71; -503.323517724
72; -120.936118882
73; -630.67021497
74; -646.061260908
75; -607.007267324
76; -359.650531738
77; -354.755749836
78; -768.901533703
79; -123.397765822
80; -504.032259719
81; -125.137500143
82; -124.705966191
83; -367.280001737
84; -370.383605539
85; -870.267112972
86; -126.356472837
87; -627.651462242
88; -123.224488991
89; -120.861188894
90; -380.477154912
91; -123.817721921
92; -1.50693337534
93; -241.679886639
94; -366.223893575
95; -237.881456774
96; -123.502793629
97; -506.710277515
98; -369.271339541
99; -125.623276789
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -1039.16790941
299; -1039.14858137
399; -1016.25807597
499; -997.878787156
Time Training (500episodes);3267.3004353046417
Evaluation Episode; Reward 
0; -1435.23294281
1; -125.891728615
2; -240.225132375
3; -1059.19223534
4; -1173.3912566
5; -1630.4650031
6; -124.582275659
7; -1730.18991165
8; -1719.00194872
9; -717.592513139
10; -126.433130687
11; -128.517566438
12; -1.6962407783
13; -127.137758624
14; -124.502149273
15; -127.873103006
16; -1158.0036688
17; -239.672960996
18; -127.42479403
19; -1602.47211064
20; -1.69183585704
21; -125.128574908
22; -1056.02698623
23; -1713.61021489
24; -1287.21685658
25; -1318.60114231
26; -1683.68242449
27; -1439.85014463
28; -1137.41611086
29; -1128.76036836
30; -949.181040752
31; -1397.28449646
32; -1277.06348951
33; -1732.98785157
34; -1.79911118183
35; -1242.46850391
36; -1.72721308536
37; -1410.27992097
38; -1265.93228635
39; -127.754965324
40; -1537.64285735
41; -1640.68984122
42; -1088.54023787
43; -1370.27694574
44; -1484.5167769
45; -908.912150806
46; -1622.41705178
47; -125.925882739
48; -1650.48177581
49; -1460.00389164
50; -1317.22626987
51; -1676.5429385
52; -1530.88316478
53; -1297.75561558
54; -120.831725079
55; -2.12921218998
56; -1303.97727016
57; -949.332080377
58; -1513.75465077
59; -1280.85887266
60; -1538.14694729
61; -1220.79048612
62; -1672.42599081
63; -128.208210861
64; -1334.61108128
65; -1625.98152129
66; -1447.29618675
67; -126.855222976
68; -1572.52469466
69; -1195.52917833
70; -952.613710019
71; -716.962824229
72; -1731.44797194
73; -1065.60913901
74; -1.75669295309
75; -1737.99953471
76; -1057.10207829
77; -1301.31761814
78; -724.938508749
79; -1150.8056763
80; -1395.96168487
81; -1297.16228584
82; -986.402062943
83; -1334.16410587
84; -829.996736223
85; -1658.11887295
86; -1739.65759621
87; -893.200600449
88; -1309.00794433
89; -1.63442018946
90; -1178.78820233
91; -1463.55717371
92; -1181.14635021
93; -1209.79273578
94; -1171.90637992
95; -1619.94621953
96; -2.13794624955
97; -1090.14757956
98; -1.63969993705
99; -1705.53738397
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 16 
Episodes Spent Training; 50 Episode Eval Avg 
199; -579.214059585
299; -272.978858723
399; -240.933830346
499; -233.248814703
599; -247.118770102
numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 16 
Episodes Spent Training; 50 Episode Eval Avg 
199; -976.738825851
299; -748.839825294
399; -333.746238222
499; -1162.01061407
599; -1109.93907219
699; -1158.15359317
799; -791.384379504
899; -963.740094212
999; -930.177178863
Time Training (1000episodes);6889.041788101196
Evaluation Episode; Reward 
0; -1487.87296456
1; -949.070015065
2; -1166.11083618
3; -1169.59870097
4; -119.202946127
5; -123.094630921
6; -1617.94991664
7; -1703.86197454
8; -1644.94247977
9; -1323.29793203
10; -960.101344667
11; -1080.90568416
12; -119.408952433
13; -1519.84622528
14; -124.578119544
15; -1285.94830473
16; -125.659141072
17; -126.45830617
18; -1389.67973772
19; -239.354116849
20; -120.02076492
21; -1265.00273626
22; -121.181147799
23; -948.428577001
24; -1058.3715826
25; -948.312851923
26; -1728.85786183
27; -944.826834859
28; -1060.3848781
29; -123.140780238
30; -125.772812377
31; -1033.03452901
32; -1697.74586146
33; -1300.4920337
34; -1055.57401034
35; -1729.77722993
36; -239.196296146
37; -125.312120566
38; -0.68312133018
39; -1345.27978134
40; -126.080859923
41; -239.463364641
42; -123.2958143
43; -125.417294077
44; -1606.4536734
45; -118.508639065
46; -119.261303313
47; -1720.62952016
48; -1353.48148648
49; -1734.20416794
50; -1207.01764435
51; -949.969109035
52; -950.048878523
53; -1364.9348058
54; -1447.42865731
55; -124.792289551
56; -1256.25548877
57; -1362.34400097
58; -1733.27036371
59; -1631.00635337
60; -1729.44820078
61; -124.423078978
62; -121.338115658
63; -1287.70599745
64; -949.171276323
65; -0.543604111924
66; -119.209487798
67; -1728.65994726
68; -239.148968878
69; -124.890129411
70; -1253.76601159
71; -1326.30069311
72; -1577.54682074
73; -1571.91559904
74; -1508.20722499
75; -1435.78740163
76; -122.022470539
77; -125.227045053
78; -1475.95770681
79; -123.24658645
80; -0.9600285328
81; -0.559405166819
82; -1627.64247802
83; -1364.05166562
84; -1352.20792046
85; -124.973305852
86; -1429.64277869
87; -1734.45445194
88; -1289.60494272
89; -125.210469482
90; -124.387537818
91; -1551.38043621
92; -123.642568952
93; -239.652486589
94; -1104.83506813
95; -836.160603448
96; -1192.45332179
97; -121.56925288
98; -1392.05182951
99; -239.389260656
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 16 
Episodes Spent Training; 50 Episode Eval Avg 
numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -1251.08981837
299; -1227.26017044
399; -800.914201623
499; -668.769588724
Time Training (500episodes);3298.3070278167725
Evaluation Episode; Reward 
0; -126.913082055
1; -1323.63446493
2; -1372.68356688
3; -1324.46310148
4; -123.639996183
5; -1336.77387276
6; -2.18920874872
7; -474.719668409
8; -1351.31152476
9; -125.297008299
10; -1311.77752165
11; -1377.50900834
12; -122.230292155
13; -1345.57004715
14; -245.950293717
15; -247.111750659
16; -1736.39151728
17; -368.035943576
18; -2.04949934529
19; -1359.19629167
20; -1259.21129151
21; -1384.30121584
22; -122.520162851
23; -125.667952891
24; -365.968208912
25; -127.548808412
26; -125.268853085
27; -1375.05128741
28; -2.23423620872
29; -1333.50011543
30; -120.850375118
31; -2.02827163547
32; -2.02097840298
33; -1350.06956327
34; -1375.4313259
35; -246.359388718
36; -1349.64998692
37; -530.1944163
38; -126.686790793
39; -1259.10612932
40; -1352.38938703
41; -1366.42145902
42; -1357.67244277
43; -1324.10198172
44; -520.335315507
45; -121.014402067
46; -250.345672337
47; -122.442669872
48; -1336.02256981
49; -1331.07431322
50; -125.217370923
51; -240.441109609
52; -2.09758824449
53; -124.376159086
54; -240.580175274
55; -1314.42047292
56; -605.043854456
57; -1307.31999083
58; -663.250152993
59; -244.225216121
60; -613.437120286
61; -468.604704354
62; -1354.39098979
63; -124.309254478
64; -1394.72739343
65; -127.598267293
66; -1321.25649548
67; -123.932061864
68; -613.250913919
69; -124.367519181
70; -127.384285937
71; -1334.28822046
72; -1735.28289042
73; -1370.29422222
74; -493.52994779
75; -1382.74949399
76; -361.129552067
77; -127.995282015
78; -126.571395954
79; -468.654032247
80; -126.991380711
81; -127.332049228
82; -1318.63449336
83; -1334.35707364
84; -1327.15647698
85; -122.051906149
86; -1360.72432067
87; -122.386503977
88; -120.795815581
89; -1311.8462986
90; -127.49912407
91; -1363.98073094
92; -1311.84412384
93; -499.637281491
94; -245.318125314
95; -1307.52354212
96; -1330.73224513
97; -368.260877463
98; -2.24725661714
99; -1318.89120758
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -974.206171438
299; -528.352953788
399; -460.796295759
Time Training (500episodes);2538.741459608078
Evaluation Episode; Reward 
0; -352.903976672
1; -121.07705149
2; -126.811045598
3; -724.684117647
4; -237.158713626
5; -126.510228572
6; -354.803199299
7; -591.482575084
8; -358.062261022
9; -248.80499465
10; -352.394059329
11; -242.632910529
12; -126.36458934
13; -358.408653875
14; -619.916408926
15; -0.268903726685
16; -603.678735337
17; -253.087401919
18; -0.0707979523953
19; -243.702719828
20; -614.623868621
21; -126.98051907
22; -353.640885958
23; -473.455967821
24; -241.24776238
25; -247.833057258
26; -236.092457142
27; -363.358379584
28; -243.182150428
29; -354.281699201
30; -245.936353167
31; -361.392717463
32; -125.941709574
33; -631.269167208
34; -243.411206003
35; -241.972260745
36; -126.379765084
37; -119.334444092
38; -0.69938450303
39; -248.968660983
40; -901.463826207
41; -243.62446396
42; -118.602922654
43; -610.890451277
44; -497.924565864
45; -236.23827364
46; -613.614543455
47; -123.837343454
48; -248.376346221
49; -248.763494203
50; -487.838267172
51; -482.165979508
52; -238.622908621
53; -246.27177642
54; -247.813937777
55; -482.636778389
56; -241.683933463
57; -484.071019559
58; -355.700908398
59; -794.246040861
60; -241.320145471
61; -604.379769307
62; -803.680303382
63; -494.44541418
64; -354.743407115
65; -120.937096018
66; -123.690381092
67; -481.571243476
68; -0.097354011753
69; -363.066629465
70; -124.990051685
71; -124.554814149
72; -0.745683776482
73; -122.781120405
74; -236.628261567
75; -119.817032486
76; -118.969349246
77; -118.272931453
78; -492.250640968
79; -0.243828140021
80; -360.651531873
81; -360.194532173
82; -364.52420304
83; -720.72242675
84; -244.714363021
85; -365.767199768
86; -123.184891157
87; -636.797549011
88; -491.498143449
89; -237.291918497
90; -479.871210255
91; -250.098756174
92; -484.284787141
93; -240.32841909
94; -249.336955086
95; -247.637386148
96; -601.767204075
97; -250.671319402
98; -1729.64202735
99; -247.071000373
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -1308.99973035
299; -1040.56657057
399; -694.004932865
499; -760.073504998
Time Training (500episodes);3255.2360405921936
Evaluation Episode; Reward 
0; -1331.50107851
1; -122.254104164
2; -1367.54601256
3; -568.925365281
4; -1325.826652
5; -621.486729568
6; -631.462684222
7; -1346.57313989
8; -1334.24514593
9; -119.029542696
10; -235.754258513
11; -1357.39791579
12; -121.930627509
13; -122.239932447
14; -356.737284782
15; -118.748916421
16; -1331.97481386
17; -0.0451670825735
18; -126.29664134
19; -126.352692441
20; -616.246120962
21; -121.049451995
22; -125.26688687
23; -122.489519072
24; -1329.66229905
25; -1319.98736583
26; -1343.10397028
27; -481.995854873
28; -622.918481525
29; -231.65411032
30; -1335.91357545
31; -1361.8419155
32; -123.728637302
33; -1335.8605395
34; -1321.00416244
35; -121.029833495
36; -1386.41475295
37; -1735.32231238
38; -1335.92387314
39; -0.0815804288656
40; -367.143561111
41; -0.164021304909
42; -1334.73477592
43; -1330.09741327
44; -1365.27578602
45; -1371.5076631
46; -1340.35058496
47; -547.214518843
48; -1332.31288569
49; -1366.83172451
50; -244.77816858
51; -1355.13902756
52; -125.7698202
53; -690.714195868
54; -478.010912519
55; -480.043214601
56; -1735.91440146
57; -1339.70540505
58; -122.6875522
59; -1333.47321885
60; -121.554452969
61; -1347.78996872
62; -121.513303483
63; -1352.74407229
64; -120.388621185
65; -1284.79574687
66; -1324.94342988
67; -1356.83356551
68; -1357.60974458
69; -649.901095379
70; -123.517115737
71; -1321.4782469
72; -1360.26218368
73; -610.796853643
74; -1730.47701492
75; -1326.05564104
76; -1377.57949668
77; -124.804724549
78; -1323.46842397
79; -126.564713385
80; -485.453255808
81; -1374.50268714
82; -1327.02173127
83; -1362.71811888
84; -485.960854414
85; -234.95709635
86; -120.803749397
87; -122.005174823
88; -1359.59258682
89; -0.237630289498
90; -1360.93573364
91; -1308.4151828
92; -1315.24998426
93; -124.88092721
94; -126.344625252
95; -125.595905161
96; -126.310020295
97; -1358.68138993
98; -1309.87220081
99; -0.177272008506
endExperiment

numstates; 100 
 num value iterations; 30 
 conv type; circular 
conv width; 11 
 channel_i; 1 
 channel_q; 15 
 hidden layer1 size; 30 
hidden layer2 size; 30 
 velocity to reward hidden layer size; 100 
obs to state hidden layer size; 100 
 learning rate; 0.000100 
TAU; 0.001000 
 batch size; 64 
Episodes Spent Training; 50 Episode Eval Avg 
199; -1197.89617178
299; -979.511643407
399; -1137.66761801
499; -921.95986065
Time Training (500episodes);3295.5498130321503
Evaluation Episode; Reward 
0; -1062.43691317
1; -1584.80037968
2; -1729.28420022
3; -1636.35339099
4; -1382.71399138
5; -927.358196165
6; -1116.16579949
7; -1647.41280151
8; -990.605865191
9; -1068.29374244
10; -1169.4796096
11; -0.310066247572
12; -1477.46235427
13; -1608.68556654
14; -1159.26038797
15; -1730.58967619
16; -1519.58516463
17; -1602.4450456
18; -120.736450511
19; -124.638222511
20; -835.574975159
21; -1360.49720641
22; -1523.64079987
23; -835.916042381
24; -1597.69051064
25; -950.377643308
26; -1396.15301758
27; -1093.53777904
28; -1526.40047959
29; -1683.10098649
30; -121.025155674
31; -1.22875812904
32; -1733.3528869
33; -1501.39802208
34; -1480.80961659
35; -1271.65597811
36; -1170.58845698
37; -851.895236475
38; -1417.86385381
39; -123.388211001
40; -1593.08240069
41; -125.59593974
42; -977.873026896
43; -1306.91869301
44; -1196.61999588
45; -1737.09354276
46; -1294.45056106
47; -125.154273271
48; -126.715957327
49; -1384.69015429
50; -120.592262354
51; -1459.26163181
52; -0.729545773763
53; -119.842109736
54; -1412.64098027
55; -1194.26905945
56; -1301.47723766
57; -1455.79272138
58; -1146.56687261
59; -1059.86200601
60; -1042.05990934
61; -1498.43874433
62; -1180.1227332
63; -127.157986783
64; -950.193380212
65; -1153.99707413
66; -817.656748209
67; -1443.54214991
68; -1465.15191841
69; -1278.30645324
70; -0.344142197153
71; -1730.45404781
72; -1532.30387526
73; -1387.2391302
74; -124.929157633
75; -1158.64243278
76; -901.021375916
77; -1146.23901385
78; -1364.71775334
79; -1440.12040718
80; -0.922610258362
81; -1705.87693112
82; -1547.48746202
83; -239.081306222
84; -1565.72392352
85; -1098.26688502
86; -1419.15552343
87; -1329.98773133
88; -1268.47576031
89; -0.364946480223
90; -1168.32941377
91; -1385.24422779
92; -1256.05335747
93; -1731.60922636
94; -1406.70031841
95; -487.300089402
96; -1540.41589748
97; -1694.79071916
98; -120.036967253
99; -120.788230489
endExperiment

